{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Data Teller","text":"<p>Data Teller is where the mystique of fortune-telling meets the precision of modern data science. Our project, founded on the principles of \"Datamancy,\" utilizes cutting-edge machine learning and artificial intelligence techniques to decipher vast datasets, revealing predictions about future trends and events with remarkable accuracy.</p> <p></p>"},{"location":"#vision","title":"Vision","text":"<p>To become the pioneering force in the field of predictive analytics, revolutionizing how we understand and prepare for future possibilities.</p>"},{"location":"#mission","title":"Mission","text":"<p>To provide clients with powerful insights derived from sophisticated data analysis, enabling informed decision-making and enhancing strategic planning.</p>"},{"location":"#what-we-offer","title":"What We Offer","text":"<ul> <li>Predictive Insights: Using advanced algorithms, Data Teller delves deep   into data to uncover what lies ahead, ensuring that you are always ahead of   the curve.</li> <li>Custom Solutions: Recognizing the unique challenges faced by each client,   we tailor our predictive models to meet your specific needs, ensuring   relevance and precision.</li> <li>Evolutionary Learning: Our tools are continuously refined as they learn   from new data, improving their predictive accuracy over time and adapting to   changing circumstances.</li> </ul> <p>Join us at Data Teller, where ancient divination meets modern data science in the quest to unveil the future. Discover how our expertise in Datamancy can guide you toward making better choices today for a more successful tomorrow.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/datateller-org.github.io/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \u201cbug\u201d and \u201chelp wanted\u201d is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \u201cenhancement\u201d and \u201chelp wanted\u201d is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>Data Teller could always use more documentation, whether as part of the official Data Teller docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/datateller-org.github.io/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are   welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here\u2019s how to set up <code>datateller-org.github.io</code> for local development.</p> <ol> <li>Fork the <code>datateller-org.github.io</code> repo on GitHub.</li> <li>Clone your fork locally:</li> </ol> <pre><code>$ git clone git@github.com:your_name_here/datateller-org.github.io.git\n</code></pre> <ol> <li>Create an environment with <code>conda</code> and install the dependencies:</li> </ol> <pre><code>$ cd datateller-org.github.io/\n$ mamba env create --file conda/dev.yaml\n$ conda activate datateller-web\n</code></pre> <p>Install the dependencies:</p> <pre><code>$ poetry install --no-root\n</code></pre> <ol> <li>Create a branch for local development:</li> </ol> <pre><code>git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally. 5. <code>datateller-org.github.io</code> uses a set of <code>pre-commit</code> hooks and the <code>pre-commit</code> bot to format, type-check, and prettify the codebase. The hooks can be installed locally using:</p> <pre><code>pre-commit install\n</code></pre> <p>This would run the checks every time a commit is created locally. The checks will only run on the files modified by that commit, but the checks can be triggered for all the files using:</p> <pre><code>$ pre-commit run --all-files\n</code></pre> <p>If you would like to skip the failing checks and push the code for further discussion, use the <code>--no-verify</code> option with <code>git commit</code>. 6. <code>datateller-org.github.io</code> is tested with <code>pytest</code>. <code>pytest</code> is responsible for testing the code, whose configuration is available in pyproject.toml. 7. Commit your changes and push your branch to GitHub::</p> <pre><code>$ git add .\n$ git commit -m \u201cYour detailed description of your changes.\u201d\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> <ol> <li>Submit a pull request through the GitHub website.</li> </ol>"},{"location":"contributing/#run-jupyter-lab","title":"Run jupyter-lab","text":"<p>All the data exploration and analysis is done using jupyter notebooks. In order to run it, use the following command:</p> <pre><code>$ jupyter-lab --notebook-dir docs/notebooks\n</code></pre>"},{"location":"contributing/#launch-the-web-page-locally","title":"Launch the web page locally","text":"<p>In order to lauch the web page locally with all the notebooks, run:</p> <pre><code>$ makim web.preview\n</code></pre>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put your     new functionality into a function with a docstring, and add the feature to     the list in README.rst.</li> <li>The pull request should work for Python 3.11.</li> </ol>"},{"location":"contributing/#release","title":"Release","text":"<p>This project uses semantic-release in order to cut a new release based on the commit-message.</p>"},{"location":"contributing/#commit-message-format","title":"Commit message format","text":"<p>semantic-release uses the commit messages to determine the consumer impact of changes in the codebase. Following formalized conventions for commit messages, semantic-release automatically determines the next semantic version number, generates a changelog and publishes the release.</p> <p>By default, semantic-release uses Angular Commit Message Conventions. The commit message format can be changed with the <code>preset</code> or <code>config</code> options_ of the @semantic-release/commit-analyzer and @semantic-release/release-notes-generator plugins.</p> <p>Tools such as commitizen or commitlint can be used to help contributors and enforce valid commit messages.</p> <p>The table below shows which commit message gets you which release type when <code>semantic-release</code> runs (using the default configuration):</p> Commit message Release type <code>fix(pencil): stop graphite breaking when pressure is applied</code> Fix Release <code>feat(pencil): add 'graphiteWidth' option</code> Feature Release <code>perf(pencil): remove graphiteWidth option</code> Chore <code>fix(pencil)!: The graphiteWidth option has been removed</code> Breaking Release <p>source: https://github.com/semantic-release/semantic-release/blob/master/README.md#commit-message-format</p> <p>As this project uses the <code>squash and merge</code> strategy, ensure to apply the commit message format to the PR's title.</p>"},{"location":"datamancy/titanic/","title":"Table of Contents","text":"In\u00a0[1]: Copied! <pre>!pip install -q numpy scikit-learn ipython matplotlib pandas sh\n</pre> !pip install -q numpy scikit-learn ipython matplotlib pandas sh In\u00a0[2]: Copied! <pre># This Python 3 environment comes with many helpful analytics libraries \n# installed\n# It is defined by the kaggle/python docker image: \n# https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nfrom IPython.display import display\nfrom matplotlib import pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import f1_score, accuracy_score, precision_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import (\n    FunctionTransformer, OneHotEncoder, StandardScaler,\n    MinMaxScaler, MaxAbsScaler\n)\nfrom sklearn.svm import SVC\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sklearn\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) \n# will list the files in the input directory\n\nimport sh\nprint(sh.ls('./input'))\n</pre> # This Python 3 environment comes with many helpful analytics libraries  # installed # It is defined by the kaggle/python docker image:  # https://github.com/kaggle/docker-python # For example, here's several helpful packages to load in  from IPython.display import display from matplotlib import pyplot as plt from sklearn.decomposition import PCA from sklearn.ensemble import RandomForestClassifier from sklearn.feature_extraction import DictVectorizer from sklearn.impute import SimpleImputer from sklearn.metrics import f1_score, accuracy_score, precision_score from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline from sklearn.preprocessing import (     FunctionTransformer, OneHotEncoder, StandardScaler,     MinMaxScaler, MaxAbsScaler ) from sklearn.svm import SVC  import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import sklearn  # Input data files are available in the \"../input/\" directory. # For example, running this (by clicking run or pressing Shift+Enter)  # will list the files in the input directory  import sh print(sh.ls('./input')) <pre>genderclassmodel.csv  gendermodel.csv  test.csv  train.csv  train.csv.ori\n\n</pre> In\u00a0[3]: Copied! <pre>Y_test =  pd.read_csv('./input/genderclassmodel.csv')\ntest =  pd.read_csv('./input/test.csv')\ntrain =  pd.read_csv('./input/train.csv')\n\nprint('test set has %s rows and %s columns.' % test.shape)\nprint('test set has %s rows and %s columns.' % Y_test.shape)\nprint('train set has %s rows and %s columns.' % train.shape)\n</pre> Y_test =  pd.read_csv('./input/genderclassmodel.csv') test =  pd.read_csv('./input/test.csv') train =  pd.read_csv('./input/train.csv')  print('test set has %s rows and %s columns.' % test.shape) print('test set has %s rows and %s columns.' % Y_test.shape) print('train set has %s rows and %s columns.' % train.shape) <pre>test set has 418 rows and 11 columns.\ntest set has 418 rows and 2 columns.\ntrain set has 891 rows and 12 columns.\n</pre> In\u00a0[4]: Copied! <pre>train.columns\n</pre> train.columns Out[4]: <pre>Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')</pre> In\u00a0[5]: Copied! <pre>def prepare_df(df, p_unique=0.2):\n    df = df.copy()\n    \n    size = len(df)\n    \n    df = pd.get_dummies(df, drop_first=True)\n    cols = list(df.columns)\n    \n    for c in cols:\n        if df[c].dtype == object:\n            if df[c].nunique()/size &gt; p_unique:\n                df.drop(c, axis=1, inplace=True)\n                continue\n            \n    return df\n</pre> def prepare_df(df, p_unique=0.2):     df = df.copy()          size = len(df)          df = pd.get_dummies(df, drop_first=True)     cols = list(df.columns)          for c in cols:         if df[c].dtype == object:             if df[c].nunique()/size &gt; p_unique:                 df.drop(c, axis=1, inplace=True)                 continue                  return df In\u00a0[6]: Copied! <pre>pca = make_pipeline(\n    SimpleImputer(missing_values=np.nan, strategy='mean'),\n    StandardScaler(),\n    PCA(random_state=42)\n)\n\npca.fit_transform(prepare_df(train.drop('Survived', axis=1)), train.Survived);\n</pre> pca = make_pipeline(     SimpleImputer(missing_values=np.nan, strategy='mean'),     StandardScaler(),     PCA(random_state=42) )  pca.fit_transform(prepare_df(train.drop('Survived', axis=1)), train.Survived); In\u00a0[7]: Copied! <pre>pca.get_params()\n</pre> pca.get_params() Out[7]: <pre>{'memory': None,\n 'steps': [('simpleimputer', SimpleImputer()),\n  ('standardscaler', StandardScaler()),\n  ('pca', PCA(random_state=42))],\n 'verbose': False,\n 'simpleimputer': SimpleImputer(),\n 'standardscaler': StandardScaler(),\n 'pca': PCA(random_state=42),\n 'simpleimputer__add_indicator': False,\n 'simpleimputer__copy': True,\n 'simpleimputer__fill_value': None,\n 'simpleimputer__keep_empty_features': False,\n 'simpleimputer__missing_values': nan,\n 'simpleimputer__strategy': 'mean',\n 'standardscaler__copy': True,\n 'standardscaler__with_mean': True,\n 'standardscaler__with_std': True,\n 'pca__copy': True,\n 'pca__iterated_power': 'auto',\n 'pca__n_components': None,\n 'pca__n_oversamples': 10,\n 'pca__power_iteration_normalizer': 'auto',\n 'pca__random_state': 42,\n 'pca__svd_solver': 'auto',\n 'pca__tol': 0.0,\n 'pca__whiten': False}</pre> In\u00a0[8]: Copied! <pre>train_ = train.copy()\n\ntrain_.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n# train_.dropna(inplace=True)\n\nX_train = prepare_df(train_.drop('Survived', axis=1))\ny_train = train_[['Survived']]\n</pre> train_ = train.copy()  train_.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True) # train_.dropna(inplace=True)  X_train = prepare_df(train_.drop('Survived', axis=1)) y_train = train_[['Survived']] In\u00a0[9]: Copied! <pre>test_ = pd.concat([Y_test, test.iloc[:, 1:]], axis=1)\n\ntest_.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n# test_.dropna(inplace=True)\n\nX_test = prepare_df(test_.drop('Survived', axis=1))\ny_test = test_[['Survived']]\n</pre> test_ = pd.concat([Y_test, test.iloc[:, 1:]], axis=1)  test_.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True) # test_.dropna(inplace=True)  X_test = prepare_df(test_.drop('Survived', axis=1)) y_test = test_[['Survived']] In\u00a0[10]: Copied! <pre>test_.columns\n</pre> test_.columns Out[10]: <pre>Index(['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',\n       'Embarked'],\n      dtype='object')</pre> In\u00a0[11]: Copied! <pre>print('X_test set has %s rows and %s columns.' % X_test.shape)\nprint('y_test set has %s rows and %s columns.' % y_test.shape)\nprint('X_train set has %s rows and %s columns.' % X_train.shape)\nprint('Y_train set has %s rows and %s columns.' % y_train.shape)\n</pre> print('X_test set has %s rows and %s columns.' % X_test.shape) print('y_test set has %s rows and %s columns.' % y_test.shape) print('X_train set has %s rows and %s columns.' % X_train.shape) print('Y_train set has %s rows and %s columns.' % y_train.shape) <pre>X_test set has 418 rows and 8 columns.\ny_test set has 418 rows and 1 columns.\nX_train set has 891 rows and 8 columns.\nY_train set has 891 rows and 1 columns.\n</pre> In\u00a0[12]: Copied! <pre># , MaxAbsScaler\npipeline_rfc = make_pipeline(\n    SimpleImputer(missing_values=np.nan, strategy='mean'),\n    MaxAbsScaler(),\n    RandomForestClassifier(random_state=42)\n)\n\npipeline_svc = make_pipeline(\n    SimpleImputer(missing_values=np.nan, strategy='mean'),\n    MaxAbsScaler(),\n    SVC()\n)\n</pre> # , MaxAbsScaler pipeline_rfc = make_pipeline(     SimpleImputer(missing_values=np.nan, strategy='mean'),     MaxAbsScaler(),     RandomForestClassifier(random_state=42) )  pipeline_svc = make_pipeline(     SimpleImputer(missing_values=np.nan, strategy='mean'),     MaxAbsScaler(),     SVC() ) In\u00a0[13]: Copied! <pre>for title, model in {\n    'RandomForestClassifier': pipeline_rfc, \n    'SVC': pipeline_svc\n}.items():\n    print('\\n%s' % model)\n    model.fit(X_train, y_train)\n\n    predicted = model.predict(X_test)\n    print('f1 score')\n    print(f1_score(y_test, predicted))\n    print('precision score')\n    print(precision_score(y_test, predicted))\n    print('accuracy score')\n    print(accuracy_score(y_test, predicted))\n    f_name = 'output/%s.csv' % title\n    \n    # np.savetxt(f_name, predicted)\n    # print('%s saved!' % f_name)\n</pre> for title, model in {     'RandomForestClassifier': pipeline_rfc,      'SVC': pipeline_svc }.items():     print('\\n%s' % model)     model.fit(X_train, y_train)      predicted = model.predict(X_test)     print('f1 score')     print(f1_score(y_test, predicted))     print('precision score')     print(precision_score(y_test, predicted))     print('accuracy score')     print(accuracy_score(y_test, predicted))     f_name = 'output/%s.csv' % title          # np.savetxt(f_name, predicted)     # print('%s saved!' % f_name) <pre>\nPipeline(steps=[('simpleimputer', SimpleImputer()),\n                ('maxabsscaler', MaxAbsScaler()),\n                ('randomforestclassifier',\n                 RandomForestClassifier(random_state=42))])\n</pre> <pre>/home/xmn/miniforge3/envs/datateller-web/lib/python3.11/site-packages/sklearn/base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  return fit_method(estimator, *args, **kwargs)\n</pre> <pre>f1 score\n0.75\nprecision score\n0.7161290322580646\naccuracy score\n0.8229665071770335\n\nPipeline(steps=[('simpleimputer', SimpleImputer()),\n                ('maxabsscaler', MaxAbsScaler()), ('svc', SVC())])\nf1 score\n0.873015873015873\nprecision score\n0.990990990990991\naccuracy score\n0.9234449760765551\n</pre> <pre>/home/xmn/miniforge3/envs/datateller-web/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n</pre>"},{"location":"datamancy/titanic/#table-of-contents","title":"Table of Contents\u00b6","text":"<p>1\u00a0\u00a0VARIABLE DESCRIPTIONS: </p>"},{"location":"datamancy/titanic/#setup","title":"Setup\u00b6","text":""},{"location":"datamancy/titanic/#variable-descriptions","title":"VARIABLE DESCRIPTIONS:\u00b6","text":"<ul> <li>survival        Survival            (0 = No; 1 = Yes)</li> <li>pclass          Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)</li> <li>name            Name</li> <li>sex             Sex</li> <li>age             Age</li> <li>sibsp           Number of Siblings/Spouses Aboard</li> <li>parch           Number of Parents/Children Aboard</li> <li>ticket          Ticket Number</li> <li>fare            Passenger Fare</li> <li>cabin           Cabin</li> <li>embarked        Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)</li> </ul> <p>SPECIAL NOTES: Pclass is a proxy for socio-economic status (SES) 1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower</p> <p>Age is in Years; Fractional if Age less than One (1) If the Age is Estimated, it is in the form xx.5</p> <p>With respect to the family relation variables (i.e. sibsp and parch) some relations were ignored.  The following are the definitions used for sibsp and parch.</p> <p>Sibling:  Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic Spouse:   Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored) Parent:   Mother or Father of Passenger Aboard Titanic Child:    Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic</p> <p>Other family relatives excluded from this study include cousins, nephews/nieces, aunts/uncles, and in-laws.  Some children travelled only with a nanny, therefore parch=0 for them.  As well, some travelled with very close friends or neighbors in a village, however, the definitions do not support such relations.</p>"}]}